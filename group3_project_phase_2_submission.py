# -*- coding: utf-8 -*-
"""Group3_Project Phase 2_Submission.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uhvhl2eDgsW7Hi1BG0gcVwLQXzqk7AXm

# Bias in Advertising - IBM Developer Data Asset Exchange

## Aim of the Dataset

* To demonstrate discovery, measurement, and mitigation of bias in advertising.
* Develop a Targeting plan based on the inferences/relations between the conversions and the features.
* Showing the particular ad to those users who have these attributes.

## Description of the features in the Dataset

1. Age
2. Gender
3. Income
4. Political / Religious Affiliation
5. Parental Status
6. Home Ownership
7. Area (Rural/Urban)
8. Education Status

## Target Variable in the Dataset - Conversion

* A user is considered to have converted (true conversion=1) if they clicked on the ad.
* Predicted_Conversion, True_Conversion (Estimated, Actual) data given
* Predicted_Conversion is obtained by thresholding the predicted probability, provided in the dataset
"""

import pandas as pd

data = pd.read_csv('ad_campaign_data.csv')
data

data.info()

"""### Unique Values in each of the Features of the Dataset"""

# Iterate through each column and print unique values
for column in data.columns:
    unique_values = data[column].unique()
    print(f"\nColumn: {column}")
    print(f"  Number of unique values: {len(unique_values)}")
    print(f"  Unique values: {unique_values}")

"""### Finding the number of Unknown / Missing values in each column"""

unknown_counts = data.apply(lambda x: x.value_counts().get('Unknown', 0))
total_rows = len(data)
unknown_percentages = (unknown_counts / total_rows) * 100
print("Number of 'Unknown' values in each column:")
print(unknown_counts)
print("\nPercentage of 'Unknown' values in each column:")
unknown_percentages

"""### Occurences / Datapoints where the Conversion differs despite being same on all other features"""

# Count occurrences where all columns except `true_conversion` and `predicted_conversion` are the same, but `true_conversion` differs
grouped_true_diff = data.groupby(['religion', 'politics', 'college_educated', 'parents', 'homeowner', 'gender', 'age', 'income', 'area'])['true_conversion']
true_diff_count = grouped_true_diff.nunique().reset_index()
true_diff_count = true_diff_count[true_diff_count['true_conversion'] > 1]
num_true_diff = true_diff_count.shape[0]

print(f"Number of instances where `true_conversion` differs despite identical other parameters: {num_true_diff}")

# Count occurrences where all columns except `true_conversion` and `predicted_conversion` are the same, but `predicted_conversion` differs
grouped_pred_diff = data.groupby(['religion', 'politics', 'college_educated', 'parents', 'homeowner', 'gender', 'age', 'income', 'area'])['predicted_conversion']
pred_diff_count = grouped_pred_diff.nunique().reset_index()
pred_diff_count = pred_diff_count[pred_diff_count['predicted_conversion'] > 1]
num_pred_diff = pred_diff_count.shape[0]

print(f"Number of instances where `predicted_conversion` differs despite identical other parameters: {num_pred_diff}")

"""### Comparison of True vs. Predicted Conversion from the Dataset"""

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix
import pandas as pd

# Calculate value counts for true_conversion and predicted_conversion
true_conversion_counts = data['true_conversion'].value_counts()
predicted_conversion_counts = data['predicted_conversion'].value_counts()

# Print value counts
print("Value Counts:")
print(true_conversion_counts, "\n")
print(predicted_conversion_counts)

# Confusion Matrix
conf_matrix = confusion_matrix(data['true_conversion'], data['predicted_conversion'])
conf_matrix_df = pd.DataFrame(conf_matrix, index=['Actual 0', 'Actual 1'], columns=['Predicted 0', 'Predicted 1'])

# Plotting the Confusion Matrix
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix_df, annot=True, fmt='d', cmap='Blues')
plt.title('Confusion Matrix of True vs. Predicted Conversions')
plt.ylabel('Actual')
plt.xlabel('Predicted')
plt.show()

"""### Predicted Probability Threshold for Conversion (1)"""

# Find the threshold
threshold = data[data['predicted_conversion'] == 1]['predicted_probability'].min()

# Print the threshold value
print(f"The threshold for predicted_probability where predicted_conversion is 1 is: {threshold}")

data = data.drop('predicted_probability', axis=1)

# Print the updated dataset
data

# Drop duplicate rows
data2 = data.drop_duplicates()
data2

"""### Value Counts in each of the features"""

from matplotlib import pyplot as plt
import seaborn as sns

# Set a color palette
palette = sns.color_palette('Dark2')

# Loop through each column in the dataset
for col in data2.columns:
    if data2[col].dtype == 'object':  # Only plot categorical columns
        plt.figure(figsize=(8, 5))

        # Get the value counts for the column
        counts = data2[col].value_counts()

        # Plot using different colors for each category in the column
        counts.plot(
            kind='barh',
            color=[palette[i % len(palette)] for i in range(len(counts))],
            edgecolor='black'
        )

        # Customize the plot
        plt.title(f'Distribution of {col}', fontsize=14)
        plt.xlabel('Count', fontsize=12)
        plt.ylabel(col, fontsize=12)

        # Remove top and right spines
        plt.gca().spines[['top', 'right']].set_visible(False)

        # Show the plot
        plt.tight_layout()
        plt.show()

unknown_counts = data2.apply(lambda x: x.value_counts().get('Unknown', 0))
total_rows = len(data2)
unknown_percentages = (unknown_counts / total_rows) * 100
print("Number of 'Unknown' values in each column:")
print(unknown_counts)
print("\nPercentage of 'Unknown' values in each column:")
unknown_percentages

for col in data2.columns:
    # Calculate percentage of unique values
    unique_counts = data2[col].value_counts(normalize=True) * 100

    # Create pie chart
    plt.figure(figsize=(8, 6))
    plt.pie(unique_counts, labels=unique_counts.index, autopct='%1.1f%%', startangle=90)
    plt.title(f'Percentage of Unique Values in {col} Column')
    plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.
    plt.show()

"""### Handling Missing Data

As the missing data is very high, instead of dropping the column, we are replacing the values of "Unknown" in the dataset based on the proportion of other unique values in that specific column / feature of the dataset
"""

import pandas as pd

# Dictionary to store the count of 'Unknown' values filled for each column
unknown_counts = {}

for col in data2.columns:
    if data2[col].dtype == 'object':  # Only process categorical columns
        # Get the distribution of values, excluding 'Unknown'
        value_counts = data2[col].value_counts(normalize=True).drop('Unknown', errors='ignore')

        # Count the number of 'Unknown' values in the column
        unknown_count = data2[col].value_counts().get('Unknown', 0)

        if unknown_count > 0:
            # Sample values based on the proportions in value_counts to fill all 'Unknown' entries
            fill_values = pd.Series(
                value_counts.sample(n=unknown_count, replace=True, weights=value_counts.values).index
            )
            data2.loc[data2[col] == 'Unknown', col] = fill_values.values
            unknown_counts[col] = unknown_count  # Track the count of filled 'Unknown' values

# Display details of 'Unknown' values filled for each column
print("\nCount of 'Unknown' values filled in each column:")
for col, count in unknown_counts.items():
    print(f"{col}: {count} 'Unknown' values filled")

# Display final count of 'Unknown' values to verify filling
print("\nFinal count of 'Unknown' values in each column after filling:")
print(data2.isin(['Unknown']).sum())

data2

"""### Value Counts of Features after replacing Unknown Values"""

from matplotlib import pyplot as plt
import seaborn as sns

# Set a color palette
palette = sns.color_palette('Dark2')

# Loop through each column in the dataset
for col in data2.columns:
    if data2[col].dtype == 'object':  # Only plot categorical columns
        plt.figure(figsize=(8, 5))

        # Get the value counts for the column
        counts = data2[col].value_counts()

        # Plot using different colors for each category in the column
        counts.plot(
            kind='barh',
            color=[palette[i % len(palette)] for i in range(len(counts))],
            edgecolor='black'
        )

        # Customize the plot
        plt.title(f'Distribution of {col}', fontsize=14)
        plt.xlabel('Count', fontsize=12)
        plt.ylabel(col, fontsize=12)

        # Remove top and right spines
        plt.gca().spines[['top', 'right']].set_visible(False)

        # Show the plot
        plt.tight_layout()
        plt.show()

for col in data2.columns:
    # Calculate percentage of unique values
    unique_counts = data2[col].value_counts(normalize=True) * 100

    # Create pie chart
    plt.figure(figsize=(8, 6))
    plt.pie(unique_counts, labels=unique_counts.index, autopct='%1.1f%%', startangle=90)
    plt.title(f'Percentage of Unique Values in {col} Column')
    plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.
    plt.show()

"""### Features vs. True & Predicted Conversions"""

import matplotlib.pyplot as plt
import seaborn as sns

# Iterate through each column (excluding 'true_conversion' and 'predicted_conversion')
for column in data2.columns[:-2]:  # Exclude the last two columns
    plt.figure(figsize=(12, 6))

    # Plot for 'true_conversion'
    plt.subplot(1, 2, 1)
    sns.countplot(x=column, hue='true_conversion', data=data)
    plt.title(f'Impact of {column} on True Conversion')
    plt.xticks(rotation=45, ha='right')
    plt.tight_layout()

    # Print value counts for true_conversion within the current column's categories
    print(f"Value Counts for True Conversion within each category of {column}:\n{data.groupby(column)['true_conversion'].value_counts()}\n")


    # Plot for 'predicted_conversion'
    plt.subplot(1, 2, 2)
    sns.countplot(x=column, hue='predicted_conversion', data=data)
    plt.title(f'Impact of {column} on Predicted Conversion')
    plt.xticks(rotation=45, ha='right')
    plt.tight_layout()

    # Print value counts for predicted_conversion within the current column's categories
    print(f"Value Counts for Predicted Conversion within each category of {column}:\n{data.groupby(column)['predicted_conversion'].value_counts()}\n")


    plt.show()

"""### Impact of College Education & Home Ownership on Conversion"""

# Set plot style
sns.set(style="whitegrid")
# College Education and Homeownership vs. Conversion (Grouped Bar Plot)
plt.figure(figsize=(12, 6))
sns.barplot(data=data2, x='college_educated', y='true_conversion', hue='homeowner', palette='muted', errorbar=None)
plt.title("Impact of College Education and Homeownership on Conversion")
plt.xlabel("College Educated")
plt.ylabel("Average Conversion Rate")
plt.xticks([0, 1], ['No', 'Yes'])
plt.legend(title="Homeowner", labels=["No", "Yes"])
plt.show()

"""### Impact of College Education & Area on Conversion"""

# Set plot style
sns.set(style="whitegrid")
# Distribution of College Education and Conversion by Area
plt.figure(figsize=(12, 6))
sns.barplot(data=data2, x='area', y='true_conversion', hue='college_educated', errorbar=None, palette="Set2")
plt.title("Distribution of College Education and Conversion by Area")
plt.xlabel("Area")
plt.ylabel("Average Conversion Rate")
plt.legend(title="College Educated", labels=["No", "Yes"])
plt.show()

"""### Impact of Gender & Area on Conversion"""

# Set plot style
sns.set(style="whitegrid")
# Gender and Area Influence on Conversion (Stacked Bar Plot)
plt.figure(figsize=(10, 6))
# Compute counts for each combination
stacked_counts = data2.groupby(['gender', 'area', 'true_conversion']).size().unstack(fill_value=0)
stacked_counts.plot(kind='bar', stacked=True, color=['#FF6F61', '#6B5B95'], width=0.7)
plt.title("Gender and Area Influence on Conversion")
plt.xlabel("Gender and Area")
plt.ylabel("Count")
plt.legend(title="True Conversion", labels=["No", "Yes"])
plt.xticks(rotation=45)
plt.show()

"""### Impact of Income & Age on Conversion"""

# Set plot style
sns.set(style="whitegrid")
# Stacked Bar Chart for Conversion by Income and Age
income_age_counts = data2.groupby(['income', 'age', 'true_conversion']).size().unstack().fillna(0)
income_age_counts.plot(kind='bar', stacked=True, color=['#FF6F61', '#6B5B95'], figsize=(14, 8))
plt.title("Stacked Bar Chart for Conversion by Income and Age")
plt.xlabel("Income and Age")
plt.ylabel("Count")
plt.legend(title="True Conversion", labels=["No", "Yes"])
plt.xticks(rotation=45)
plt.show()

"""### Impact of Religion and Politics on Conversion"""

# Set plot style
sns.set(style="whitegrid")

# Extract unique labels from the 'politics' column
unique_politics_labels = data2['politics'].unique()

# Religion & Politics vs. Conversion (Grouped Bar Plot)
plt.figure(figsize=(16, 6))
sns.barplot(data=data2, x='politics', y='true_conversion', hue='religion', palette='muted', errorbar=None)

# Title and labels
plt.title("Impact of Religion and Politics on Conversion", fontsize=16)
plt.xlabel("Politics", fontsize=14)
plt.ylabel("Average Conversion Rate", fontsize=14)

# Set x-tick labels based on unique values in the 'politics' column
plt.xticks(ticks=range(len(unique_politics_labels)), labels=unique_politics_labels, fontsize=12)

# Legend adjustments
plt.legend(title="Religion", labels=["Christianity", "Others"], title_fontsize=13, fontsize=11)

plt.show()

"""### Pairplots"""

import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd

# Set the plotting style
sns.set(style="whitegrid")

# Pairplot with Conversion
plt.figure(figsize=(12, 10))
sns.pairplot(data2, hue="true_conversion", palette="viridis", markers=["o", "s"])
plt.suptitle("Pairwise Relationships of Features by Conversion Status", y=1.02)
plt.show()

"""### Stacked Area Plots"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Iterate through each categorical column
for column in data2.select_dtypes(include=['object']).columns:
    # Group by the current column and 'true_conversion'
    conversion_counts = data2.groupby([column, 'true_conversion']).size().unstack().fillna(0)

    # Normalize the counts to get proportions
    conversion_counts = conversion_counts.div(conversion_counts.sum(axis=1), axis=0)

    # Generate a color palette based on the number of unique values in the column
    num_unique_values = conversion_counts.shape[0]
    color_palette = sns.color_palette("husl", num_unique_values)  # Use the Husl palette

    # Create a stacked area plot
    plt.figure(figsize=(18, 6))  # Increase figure size
    conversion_counts.plot(kind="area", stacked=True, color=color_palette)

    # Title and labels
    plt.title(f"Stacked Area Plot of Conversion over {column.replace('_', ' ').title()}", fontsize=16)
    plt.xlabel(column.replace('_', ' ').title(), fontsize=14)
    plt.ylabel("Proportion of Conversion", fontsize=14)
    plt.legend(title="True Conversion", labels=["No", "Yes"], loc="upper right")

    # Rotate x-axis labels for better visibility
    plt.xticks(rotation=45, ha='right')

    plt.tight_layout()  # Adjust layout to prevent overlap
    plt.show()

"""### Violin Plot indicating Conversion based on Religion and Politics"""

# Religion and Politics Influence on Conversion (Violin Plot)
plt.figure(figsize=(14, 8))
sns.violinplot(data=data2, x="politics", y="true_conversion", hue="religion", split=True, inner="quartile", palette="muted")
plt.title("Religion and Politics Influence on Conversion (Violin Plot)")
plt.xlabel("Politics")
plt.ylabel("Conversion Rate")
plt.legend(title="Religion")
plt.show()

"""### Violin Plot indicating Conversion based on Gender and Politics"""

# Gender and Politics Influence on Conversion (Violin Plot)
plt.figure(figsize=(14, 8))
sns.violinplot(data=data2, x="politics", y="true_conversion", hue="gender", split=True, inner="quartile", palette="muted")
plt.title("Gender and Politics Influence on Conversion (Violin Plot)")
plt.xlabel("Politics")
plt.ylabel("Conversion Rate")
plt.legend(title="Gender")
plt.show()

"""### Violin Plot indicating Conversion based on Gender and Income"""

# Gender and Income Influence on Conversion (Violin Plot)
plt.figure(figsize=(14, 8))
sns.violinplot(data=data2, x="gender", y="true_conversion", hue="income", split=True, inner="quartile", palette="muted")
plt.title("Gender and Income Influence on Conversion (Violin Plot)")
plt.xlabel("Gender")
plt.ylabel("Conversion Rate")
plt.legend(title="Income")
plt.show()

"""### Comparison of Conversion Rates (True and Predicted) based on several parameters"""

# Conversion Rate by Age and Income (Heatmap)
plt.figure(figsize=(12, 8))
age_income_conversion = data2.pivot_table(index="age", columns="income", values="true_conversion", aggfunc="mean")
sns.heatmap(age_income_conversion, annot=True, cmap="YlGnBu", fmt=".2f", cbar_kws={'label': 'Conversion Rate'})
plt.title("Conversion Rate by Age and Income")
plt.xlabel("Income Range")
plt.ylabel("Age Range")
plt.show()

# Conversion Rate based on 'religion' and 'politics'
plt.figure(figsize=(12, 6))

# Group by religion and politics and see the mean of true_conversion
religion_politics_impact = data2.groupby(['religion', 'politics'])['true_conversion'].mean().unstack()
plt.figure(figsize=(10,6))
sns.heatmap(religion_politics_impact, annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Average True Conversion Rate by Religion and Politics')
plt.show()

# Group by religion and politics and see the mean of predicted_conversion
religion_politics_impact = data2.groupby(['religion', 'politics'])['predicted_conversion'].mean().unstack()
plt.figure(figsize=(10,6))
sns.heatmap(religion_politics_impact, annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Average Predicted Conversion Rate by Religion and Politics')
plt.show()

# Conversion Rate based on 'age' and 'area'
plt.figure(figsize=(12, 6))

# Group by age and area and see the mean of true_conversion
age_area_impact = data2.groupby(['area', 'age'])['true_conversion'].mean().unstack()
plt.figure(figsize=(10,6))
sns.heatmap(age_area_impact, annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Average True Conversion Rate by Age and Area')
plt.show()

# Group by age and area and see the mean of predicted_conversion
age_area_impact = data2.groupby(['area', 'age'])['predicted_conversion'].mean().unstack()
plt.figure(figsize=(10,6))
sns.heatmap(age_area_impact, annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Average Predicted Conversion Rate by Age and Area')
plt.show()

# Conversion Rate based on 'Gender' and 'Income'
plt.figure(figsize=(12, 6))

# Group by Gender and Income and see the mean of true_conversion
gender_income_impact = data2.groupby(['gender', 'income'])['true_conversion'].mean().unstack()
plt.figure(figsize=(10,6))
sns.heatmap(gender_income_impact, annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Average True Conversion Rate by Gender and Income')
plt.show()

# Group by Gender and Income and see the mean of predicted_conversion
gender_income_impact = data2.groupby(['gender', 'income'])['predicted_conversion'].mean().unstack()
plt.figure(figsize=(10,6))
sns.heatmap(gender_income_impact, annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Average Predicted Conversion Rate by Gender and Income')
plt.show()

# Conversion Rate based on 'religion' and 'gender'
plt.figure(figsize=(12, 6))

# Group by religion and gender and see the mean of true_conversion
religion_gender_impact = data2.groupby(['religion', 'gender'])['true_conversion'].mean().unstack()
plt.figure(figsize=(10,6))
sns.heatmap(religion_gender_impact, annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Average True Conversion Rate by Religion and Gender')
plt.show()

# Group by religion and gender and see the mean of predicted_conversion
religion_politics_impact = data2.groupby(['religion', 'gender'])['predicted_conversion'].mean().unstack()
plt.figure(figsize=(10,6))
sns.heatmap(religion_gender_impact, annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Average Predicted Conversion Rate by Religion and Gender')
plt.show()

"""### Prediction Power Score"""

# Install the ppscore library if not already installed
!pip install ppscore

# Import necessary libraries
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import ppscore as pps  # PPS library

# Convert categorical variables to 'category' dtype if not already done
categorical_cols = ["age", "religion", "income", "politics", "college_educated", "parents", "homeowner", "gender", "area"]
for col in categorical_cols:
    data2[col] = data2[col].astype("category")

# Calculate the PPS matrix
pps_matrix = pps.matrix(data2)[["x", "y", "ppscore"]].pivot(columns="x", index="y", values="ppscore")

# Plot the PPS matrix as a heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(pps_matrix, annot=True, cmap="viridis", vmin=0, vmax=1, cbar_kws={'label': 'Predictive Power Score'})
plt.title("PPS Matrix Heatmap")
plt.show()

"""# Implementation of Classification Models on Given Dataset (with Imbalance in Class)

### Decision Tree
"""

data3 = data2.drop('predicted_conversion', axis=1)
data3

import pandas as pd
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, roc_curve, auc
import matplotlib.pyplot as plt
import seaborn as sns

# Prepare the data
X = data3.drop('true_conversion', axis=1)
y = data3['true_conversion']

# Convert categorical features to numerical using one-hot encoding
X = pd.get_dummies(X, drop_first=True)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Initialize and train the decision tree classifier
clf = DecisionTreeClassifier(random_state=42)  # No max_depth for a fully grown tree
clf.fit(X_train, y_train)

# Visualize the decision tree
plt.figure(figsize=(40, 20))
plot_tree(clf, feature_names=X.columns, class_names=['0', '1'], filled=True, rounded=True)
plt.show()

# Make predictions on the test set
y_pred = clf.predict(X_test)
y_prob = clf.predict_proba(X_test)[:, 1]

# Evaluate the model
cm = confusion_matrix(y_test, y_pred)

# Plot confusion matrix heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,
            xticklabels=['Predicted 0', 'Predicted 1'],
            yticklabels=['Actual 0', 'Actual 1'])
plt.title('Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

# ROC curve
fpr, tpr, thresholds = roc_curve(y_test, y_prob)
roc_auc = auc(fpr, tpr)

plt.figure(figsize=(10, 6))
plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic')
plt.legend(loc="lower right")
plt.show()

"""### Pre-Pruned Decision Tree"""

from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.metrics import roc_auc_score, roc_curve, auc, confusion_matrix
from sklearn.model_selection import cross_val_score
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

# Initialize variables to store the best depth and the highest AUC score
best_depth = None
best_auc = 0
auc_scores = []

# Test max depths from 4 to 16
for depth in range(4, 16):
    clf = DecisionTreeClassifier(max_depth=depth, random_state=42)

    # Calculate the mean AUC score using cross-validation
    cv_auc = cross_val_score(clf, X_train, y_train, cv=5, scoring='roc_auc').mean()
    auc_scores.append(cv_auc)

    # Update best depth if current AUC is higher than the previous best
    if cv_auc > best_auc:
        best_auc = cv_auc
        best_depth = depth

# Train the decision tree with the best depth
clf_best = DecisionTreeClassifier(max_depth=best_depth, random_state=42)
clf_best.fit(X_train, y_train)

# Display the best depth
print(f"Best depth for maximum AUC: {best_depth}")

# Visualize the decision tree with the best depth
plt.figure(figsize=(20, 10))
plot_tree(clf_best, feature_names=X.columns, class_names=['0', '1'], filled=True, rounded=True)
plt.title(f"Decision Tree (Optimal Depth = {best_depth})")
plt.show()

# Make predictions and calculate AUC on the test set
y_pred = clf_best.predict(X_test)
y_prob = clf_best.predict_proba(X_test)[:, 1]
fpr, tpr, thresholds = roc_curve(y_test, y_prob)
roc_auc = auc(fpr, tpr)

# Confusion matrix
cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,
            xticklabels=['Predicted 0', 'Predicted 1'],
            yticklabels=['Actual 0', 'Actual 1'])
plt.title('Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

# Plot ROC curve
plt.figure(figsize=(10, 6))
plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic')
plt.legend(loc="lower right")
plt.show()

"""### Post-Pruned Decision Tree"""

import pandas as pd
import numpy as np
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import confusion_matrix, accuracy_score, roc_curve, auc
import matplotlib.pyplot as plt
import seaborn as sns

# Prepare the data
X = data3.drop('true_conversion', axis=1)
y = data3['true_conversion']

# Convert categorical features to numerical using one-hot encoding
X = pd.get_dummies(X, drop_first=True)

# Split the data into training and validation sets if not already done
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3, random_state=42)

# Fit a decision tree classifier to get the effective alphas
clf = DecisionTreeClassifier(random_state=42)
path = clf.cost_complexity_pruning_path(X_train, y_train)
ccp_alphas = path.ccp_alphas  # array of alpha values for pruning
clfs = []

# Train a series of decision trees with different ccp_alpha values
for ccp_alpha in ccp_alphas:
    clf = DecisionTreeClassifier(random_state=42, ccp_alpha=ccp_alpha)
    clf.fit(X_train, y_train)
    clfs.append(clf)

# Calculate training and validation accuracies for each tree
train_scores = [accuracy_score(y_train, clf.predict(X_train)) for clf in clfs]
val_scores = [accuracy_score(y_val, clf.predict(X_val)) for clf in clfs]

# Identify the best alpha that gives the maximum validation accuracy
best_alpha_index = np.argmax(val_scores)
best_alpha = ccp_alphas[best_alpha_index]

print(f"Best ccp_alpha for maximum validation accuracy: {best_alpha}")

# Train the decision tree with the best ccp_alpha
clf_best = DecisionTreeClassifier(random_state=42, ccp_alpha=best_alpha)
clf_best.fit(X_train, y_train)

# Plot training and validation scores vs. ccp_alpha
plt.figure(figsize=(10, 6))
plt.plot(ccp_alphas, train_scores, marker='o', label='Training Accuracy', drawstyle="steps-post")
plt.plot(ccp_alphas, val_scores, marker='o', label='Validation Accuracy', drawstyle="steps-post")
plt.xlabel("ccp_alpha")
plt.ylabel("Accuracy")
plt.title("Training and Validation Accuracy vs ccp_alpha")
plt.legend()
plt.show()

# Visualize the pruned decision tree
plt.figure(figsize=(20, 10))
plot_tree(clf_best, feature_names=X.columns, class_names=['0', '1'], filled=True, rounded=True)
plt.title(f"Decision Tree (Post-Pruned with ccp_alpha = {best_alpha})")
plt.show()

# Evaluate on the test set
y_pred = clf_best.predict(X_test)
test_accuracy = accuracy_score(y_test, y_pred)
print(f"Test accuracy with best ccp_alpha: {test_accuracy}")

# Confusion Matrix
cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,
            xticklabels=['Predicted 0', 'Predicted 1'],
            yticklabels=['Actual 0', 'Actual 1'])
plt.title('Confusion Matrix (Post-Pruned Decision Tree)')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

# ROC Curve
from sklearn.metrics import roc_curve, auc
fpr, tpr, thresholds = roc_curve(y_test, y_prob)
roc_auc = auc(fpr, tpr)

plt.figure(figsize=(10, 6))
plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC)')
plt.legend(loc="lower right")
plt.show()

"""### Bagging Classifier"""

from sklearn.ensemble import BaggingClassifier
from sklearn.metrics import classification_report, roc_auc_score, roc_curve, auc, confusion_matrix
from sklearn.model_selection import GridSearchCV
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

# Bagging Classifier with extended parameter grid
bagging_clf = BaggingClassifier(random_state=42)
bagging_params = {
    'n_estimators': [50, 100, 200],
    'max_samples': [0.6, 0.8, 1.0],
    'max_features': [0.5, 0.75, 1.0],
    'bootstrap': [True, False]
}
bagging_grid = GridSearchCV(bagging_clf, bagging_params, cv=5, scoring='roc_auc')
bagging_grid.fit(X_train, y_train)
bagging_best = bagging_grid.best_estimator_

# Predictions and Evaluation
bagging_pred = bagging_best.predict(X_test)
bagging_prob = bagging_best.predict_proba(X_test)[:, 1]

# Print Best Parameters and Classification Report
print("Bagging Classifier:")
print("Best parameters:", bagging_grid.best_params_)
print(classification_report(y_test, bagging_pred))
print(f"ROC AUC Score: {roc_auc_score(y_test, bagging_prob)}")

# Confusion Matrix Heatmap
conf_matrix = confusion_matrix(y_test, bagging_pred)
plt.figure(figsize=(6, 4))
sns.heatmap(conf_matrix, annot=True, fmt="d", cmap="Blues", cbar=False)
plt.title("Confusion Matrix for Bagging Classifier")
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.show()

# ROC Curve
fpr, tpr, thresholds = roc_curve(y_test, bagging_prob)
roc_auc = auc(fpr, tpr)

plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, label=f'Bagging (AUC = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], 'k--')  # Random classifier line
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve for Bagging Classifier')
plt.legend(loc='lower right')
plt.show()

"""### Random Forest Classifier"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, roc_auc_score, roc_curve, auc, confusion_matrix
from sklearn.model_selection import GridSearchCV
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# Random Forest Classifier with extended parameter grid
rf_clf = RandomForestClassifier(random_state=42)
rf_params = {
    'n_estimators': [100, 200, 300],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'max_features': ['sqrt', 'log2', None]
}
rf_grid = GridSearchCV(rf_clf, rf_params, cv=5, scoring='roc_auc')
rf_grid.fit(X_train, y_train)
rf_best = rf_grid.best_estimator_

# Predictions and Evaluation
rf_pred = rf_best.predict(X_test)
rf_prob = rf_best.predict_proba(X_test)[:, 1]

# Print Best Parameters and Classification Report
print("\nRandom Forest Classifier:")
print("Best parameters:", rf_grid.best_params_)
print(classification_report(y_test, rf_pred))
print(f"ROC AUC Score: {roc_auc_score(y_test, rf_prob)}")

# Confusion Matrix Heatmap
conf_matrix_rf = confusion_matrix(y_test, rf_pred)
plt.figure(figsize=(6, 4))
sns.heatmap(conf_matrix_rf, annot=True, fmt="d", cmap="Greens", cbar=False)
plt.title("Confusion Matrix for Random Forest Classifier")
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.show()

# ROC Curve
fpr_rf, tpr_rf, thresholds_rf = roc_curve(y_test, rf_prob)
roc_auc_rf = auc(fpr_rf, tpr_rf)

plt.figure(figsize=(8, 6))
plt.plot(fpr_rf, tpr_rf, label=f'Random Forest (AUC = {roc_auc_rf:.2f})')
plt.plot([0, 1], [0, 1], 'k--')  # Random classifier line
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve for Random Forest Classifier')
plt.legend(loc='lower right')
plt.show()

# Feature Importance
rf_feature_importances = pd.Series(rf_best.feature_importances_, index=X_train.columns).sort_values(ascending=False)
print("\nFeature Importances (Random Forest):")
print(rf_feature_importances)

# Plot feature importances
plt.figure(figsize=(10, 6))
rf_feature_importances.plot(kind='bar')
plt.title("Feature Importances in Random Forest")
plt.xlabel("Features")
plt.ylabel("Importance Score")
plt.show()

"""### Naive Bayes Classification"""

from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, roc_curve, auc
import matplotlib.pyplot as plt
import seaborn as sns

# Assuming X_train, X_test, y_train, y_test are already defined from the previous code

# Initialize and train the Gaussian Naive Bayes classifier
nb_classifier = GaussianNB()
nb_classifier.fit(X_train, y_train)

# Make predictions on the test set
y_pred = nb_classifier.predict(X_test)
y_prob = nb_classifier.predict_proba(X_test)[:, 1]

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
roc_auc = roc_auc_score(y_test, y_prob)


print(f"Accuracy: {accuracy}")
print(f"Precision: {precision}")
print(f"Recall: {recall}")
print(f"F1-score: {f1}")
print(f"ROC AUC: {roc_auc}")

# Confusion Matrix
cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,
            xticklabels=['Predicted 0', 'Predicted 1'],
            yticklabels=['Actual 0', 'Actual 1'])
plt.title('Confusion Matrix (Naive Bayes)')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

# ROC Curve
fpr, tpr, thresholds = roc_curve(y_test, y_prob)
roc_auc = auc(fpr, tpr)

plt.figure(figsize=(10, 6))
plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (Naive Bayes)')
plt.legend(loc="lower right")
plt.show()

"""### Artificial Neural Network"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from tensorflow import keras
from tensorflow.keras import layers
from sklearn.metrics import confusion_matrix, roc_curve, auc
import matplotlib.pyplot as plt
import seaborn as sns

# Assuming 'data' is your DataFrame
# Prepare the data
X = data3.drop('true_conversion', axis=1)
y = data3['true_conversion']

# Identify numerical and categorical features
numerical_cols = X.select_dtypes(include=np.number).columns
categorical_cols = X.select_dtypes(include=['object']).columns

# Create transformers
numerical_transformer = StandardScaler()
categorical_transformer = OneHotEncoder(handle_unknown="ignore")

# Combine transformers using ColumnTransformer
preprocessor = ColumnTransformer(
    transformers=[
        ('num', numerical_transformer, numerical_cols),
        ('cat', categorical_transformer, categorical_cols)
    ])


# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create a pipeline
model_pipeline = Pipeline(steps=[('preprocessor', preprocessor)])

# Fit and transform the training data
X_train_transformed = model_pipeline.fit_transform(X_train)

# Transform the testing data
X_test_transformed = model_pipeline.transform(X_test)

# Define the ANN model
model = keras.Sequential([
    layers.Dense(128, activation='relu', input_shape=(X_train_transformed.shape[1],)),
    layers.Dropout(0.1),  # Add dropout for regularization
    layers.Dense(64, activation='relu'),
    layers.Dropout(0.1),
    layers.Dense(1, activation='sigmoid')  # Output layer for binary classification
])

# Compile the model
model.compile(optimizer='adam',
              loss='binary_crossentropy',
              metrics=['accuracy'])

# Train the model
history = model.fit(X_train_transformed, y_train, epochs=100, batch_size=32, validation_split=0.1)


# Make predictions on the test set
y_pred_prob = model.predict(X_test_transformed)
y_pred = (y_pred_prob > 0.5).astype(int)

# Evaluate the model
loss, accuracy = model.evaluate(X_test_transformed, y_test, verbose=0)
print(f"Test Loss: {loss:.4f}")
print(f"Test Accuracy: {accuracy:.4f}")


#Confusion Matrix
cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,
            xticklabels=['Predicted 0', 'Predicted 1'],
            yticklabels=['Actual 0', 'Actual 1'])
plt.title('Confusion Matrix (ANN)')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

#ROC Curve
fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)
roc_auc = auc(fpr, tpr)

plt.figure(figsize=(10, 6))
plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ANN)')
plt.legend(loc="lower right")
plt.show()

# Plot training & validation accuracy values
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Model accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')
plt.show()

# Plot training & validation loss values
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')
plt.show()

"""### XG Boost"""

# Import necessary libraries
import pandas as pd
from sklearn.model_selection import train_test_split, RandomizedSearchCV
from xgboost import XGBClassifier
from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import LabelEncoder

# Separate features and target
X = data3.drop("true_conversion", axis=1)
y = data3["true_conversion"]

# Convert categorical features to numeric using one-hot encoding
X = pd.get_dummies(X, drop_first=True)

# Encode the target variable if it is categorical
le = LabelEncoder()
y = le.fit_transform(y)

# Split dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Define parameter grid for XGBoost
param_dist = {
    "n_estimators": [100, 200, 300, 400, 500],
    "max_depth": [3, 5, 7, 9, 11],
    "learning_rate": [0.01, 0.05, 0.1, 0.15, 0.2],
    "subsample": [0.6, 0.7, 0.8, 0.9, 1.0],
    "colsample_bytree": [0.6, 0.7, 0.8, 0.9, 1.0],
    "gamma": [0, 0.1, 0.2, 0.3, 0.4],
    "min_child_weight": [1, 3, 5, 7]
}

# Initialize XGBoost classifier
xgb_clf = XGBClassifier(use_label_encoder=False, eval_metric='logloss')

# Use RandomizedSearchCV to tune parameters
random_search = RandomizedSearchCV(
    estimator=xgb_clf,
    param_distributions=param_dist,
    n_iter=50,
    scoring='roc_auc',
    cv=5,
    verbose=1,
    n_jobs=-1,
    random_state=42
)

# Fit the model
random_search.fit(X_train, y_train)

# Get the best estimator
best_model = random_search.best_estimator_

# Make predictions
y_pred = best_model.predict(X_test)
y_pred_proba = best_model.predict_proba(X_test)[:, 1]

# Confusion Matrix
cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", cbar=False)
plt.title("Confusion Matrix")
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.show()

# ROC Curve
fpr, tpr, _ = roc_curve(y_test, y_pred_proba)
roc_auc = auc(fpr, tpr)

plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color="blue", label="ROC Curve (AUC = {:.2f})".format(roc_auc))
plt.plot([0, 1], [0, 1], color="red", linestyle="--")
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("Receiver Operating Characteristic (ROC) Curve")
plt.legend(loc="lower right")
plt.show()

# Print classification report
print(classification_report(y_test, y_pred))

"""# DATASET BALANCING BY SMOTE (Synthetic Minority Oversampling Technique)"""

data3

# Calculate the count and percentage of true conversions (1) and non-conversions (0)
conversion_counts = data3['true_conversion'].value_counts()
conversion_percentages = data3['true_conversion'].value_counts(normalize=True) * 100

print("Conversion Counts:\n", conversion_counts)
print("\nConversion Percentages:\n", conversion_percentages)

from imblearn.over_sampling import SMOTE
from sklearn.preprocessing import OneHotEncoder, LabelEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import FunctionTransformer
from collections import Counter
import pandas as pd

# Separate features and target
X = data3.drop('true_conversion', axis=1)  # Replace 'target_column' with the actual name of the target variable
y = data3['true_conversion']

print("Original dataset shape %s" % Counter(y))

# Step 1: Identify categorical and numeric columns
categorical_cols = X.select_dtypes(include=['object', 'category']).columns
numerical_cols = X.select_dtypes(include=['number']).columns

# Step 2: Determine the encoding strategy for each categorical column
# Threshold for deciding one-hot encoding vs label encoding
high_cardinality_threshold = 10

# Separate columns based on their cardinality (number of unique values)
low_cardinality_categorical_cols = [col for col in categorical_cols if X[col].nunique() <= high_cardinality_threshold]
high_cardinality_categorical_cols = [col for col in categorical_cols if X[col].nunique() > high_cardinality_threshold]

# Step 3: Define transformers
# 3a. One-hot encode low-cardinality columns
one_hot_transformer = OneHotEncoder(drop='first', handle_unknown='ignore')

# 3b. Label encode high-cardinality columns
# Use a custom transformer to apply LabelEncoder to each column individually
def label_encode_columns(X):
    X = X.copy()
    for col in X.columns:
        X[col] = LabelEncoder().fit_transform(X[col].astype(str))
    return X

label_transformer = FunctionTransformer(label_encode_columns)

# Step 4: Create a ColumnTransformer for preprocessing
preprocessor = ColumnTransformer(
    transformers=[
        ('num', 'passthrough', numerical_cols),  # Keep numerical columns as is
        ('low_card_cat', one_hot_transformer, low_cardinality_categorical_cols),
        ('high_card_cat', label_transformer, high_cardinality_categorical_cols)
    ],
    remainder='drop'  # Drop any columns not specified
)

# Step 5: Preprocess the data
X_preprocessed = preprocessor.fit_transform(X)

# Step 6: Apply SMOTE to the preprocessed data
sm = SMOTE(random_state=42)
X_res, y_res = sm.fit_resample(X_preprocessed, y)

print("Resampled dataset shape %s" % Counter(y_res))

# Step 7: Convert SMOTE results to DataFrames
import numpy as np

# Get the transformed feature names from the ColumnTransformer
one_hot_columns = preprocessor.named_transformers_['low_card_cat'].get_feature_names_out(low_cardinality_categorical_cols)
feature_names = numerical_cols.tolist() + one_hot_columns.tolist() + high_cardinality_categorical_cols
X_res_df = pd.DataFrame(X_res, columns=feature_names)
y_res_df = pd.Series(y_res, name='true_conversion')

# Step 8: Revert label encoding for high cardinality columns
# Define a function to reverse label encoding using the original data's unique values

def revert_label_encoding(col, original_data):
    le = LabelEncoder()
    le.fit(original_data[col].astype(str))
    return le.inverse_transform(X_res_df[col].astype(int))

for col in high_cardinality_categorical_cols:
    X_res_df[col] = revert_label_encoding(col, X)

# Step 9: Revert one-hot encoding for low cardinality columns
# This step will combine one-hot encoded columns into their original categorical columns

for col in low_cardinality_categorical_cols:
    # Get all one-hot columns for this category
    one_hot_columns = [c for c in X_res_df.columns if c.startswith(col + '_')]
    if one_hot_columns:
        # Find the original category by locating the max value per row
        X_res_df[col] = X_res_df[one_hot_columns].idxmax(axis=1).str.replace(f'{col}_', '')
        # Drop the one-hot columns as we now have the original column back
        X_res_df.drop(columns=one_hot_columns, inplace=True)

# Step 10: Combine features and target back into one DataFrame
data4 = pd.concat([X_res_df, y_res_df.reset_index(drop=True)], axis=1)

# Display the first few rows to verify the format
data4

# Calculate the count and percentage of true conversions (1) and non-conversions (0)
conversion_counts = data4['true_conversion'].value_counts()
conversion_percentages = data4['true_conversion'].value_counts(normalize=True) * 100

print("Conversion Counts:\n", conversion_counts)
print("\nConversion Percentages:\n", conversion_percentages)

"""# CLASSIFICATION MODELS AFTER SMOTE"""

data4

import pandas as pd
from sklearn.utils import shuffle

# Assuming 'data4' is your DataFrame
# Shuffle the entire dataset to mix 0s and 1s in 'true_conversion'
data5 = shuffle(data4, random_state=42)

# After shuffling, you can recheck the distribution of the classes
print("Shuffled dataset class distribution:\n", data5['true_conversion'].value_counts())

data5

"""### Decision Tree after SMOTE"""

import pandas as pd
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, roc_curve, auc
import matplotlib.pyplot as plt
import seaborn as sns

# Prepare the data
X = data5.drop('true_conversion', axis=1)
y = data5['true_conversion']

# Convert categorical features to numerical using one-hot encoding
X = pd.get_dummies(X, drop_first=True)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize and train the decision tree classifier
clf = DecisionTreeClassifier(random_state=42)  # No max_depth for a fully grown tree
clf.fit(X_train, y_train)

# Visualize the decision tree
plt.figure(figsize=(40, 20))
plot_tree(clf, feature_names=X.columns, class_names=['0', '1'], filled=True, rounded=True)
plt.show()

# Make predictions on the test set
y_pred = clf.predict(X_test)
y_prob = clf.predict_proba(X_test)[:, 1]

# Evaluate the model
cm = confusion_matrix(y_test, y_pred)

# Plot confusion matrix heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,
            xticklabels=['Predicted 0', 'Predicted 1'],
            yticklabels=['Actual 0', 'Actual 1'])
plt.title('Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

# ROC curve
fpr, tpr, thresholds = roc_curve(y_test, y_prob)
roc_auc = auc(fpr, tpr)

plt.figure(figsize=(10, 6))
plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic')
plt.legend(loc="lower right")
plt.show()

"""### Pre-Pruned Decision Tree after SMOTE"""

from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.metrics import roc_auc_score, roc_curve, auc, confusion_matrix
from sklearn.model_selection import cross_val_score
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

# Initialize variables to store the best depth and the highest AUC score
best_depth = None
best_auc = 0
auc_scores = []

# Test max depths from 4 to 16
for depth in range(4, 16):
    clf = DecisionTreeClassifier(max_depth=depth, random_state=42)

    # Calculate the mean AUC score using cross-validation
    cv_auc = cross_val_score(clf, X_train, y_train, cv=5, scoring='roc_auc').mean()
    auc_scores.append(cv_auc)

    # Update best depth if current AUC is higher than the previous best
    if cv_auc > best_auc:
        best_auc = cv_auc
        best_depth = depth

# Prepare the data
X = data5.drop('true_conversion', axis=1)
y = data5['true_conversion']

# Convert categorical features to numerical using one-hot encoding
X = pd.get_dummies(X, drop_first=True)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train the decision tree with the best depth
clf_best = DecisionTreeClassifier(max_depth=best_depth, random_state=42)
clf_best.fit(X_train, y_train)

# Display the best depth
print(f"Best depth for maximum AUC: {best_depth}")

# Visualize the decision tree with the best depth
plt.figure(figsize=(20, 10))
plot_tree(clf_best, feature_names=X.columns, class_names=['0', '1'], filled=True, rounded=True)
plt.title(f"Decision Tree (Optimal Depth = {best_depth})")
plt.show()

# Make predictions and calculate AUC on the test set
y_pred = clf_best.predict(X_test)
y_prob = clf_best.predict_proba(X_test)[:, 1]
fpr, tpr, thresholds = roc_curve(y_test, y_prob)
roc_auc = auc(fpr, tpr)

# Confusion matrix
cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,
            xticklabels=['Predicted 0', 'Predicted 1'],
            yticklabels=['Actual 0', 'Actual 1'])
plt.title('Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

# Plot ROC curve
plt.figure(figsize=(10, 6))
plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic')
plt.legend(loc="lower right")
plt.show()

"""### Post-Pruned Decision Tree after SMOTE"""

import pandas as pd
import numpy as np
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import confusion_matrix, accuracy_score, roc_curve, auc
import matplotlib.pyplot as plt
import seaborn as sns

# Prepare the data
X = data4.drop('true_conversion', axis=1)
y = data4['true_conversion']

# Convert categorical features to numerical using one-hot encoding
X = pd.get_dummies(X, drop_first=True)

# Split the data into training and validation sets if not already done
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

# Fit a decision tree classifier to get the effective alphas
clf = DecisionTreeClassifier(random_state=42)
path = clf.cost_complexity_pruning_path(X_train, y_train)
ccp_alphas = path.ccp_alphas  # array of alpha values for pruning
clfs = []

# Train a series of decision trees with different ccp_alpha values
for ccp_alpha in ccp_alphas:
    clf = DecisionTreeClassifier(random_state=42, ccp_alpha=ccp_alpha)
    clf.fit(X_train, y_train)
    clfs.append(clf)

# Calculate training and validation accuracies for each tree
train_scores = [accuracy_score(y_train, clf.predict(X_train)) for clf in clfs]
val_scores = [accuracy_score(y_val, clf.predict(X_val)) for clf in clfs]

# Identify the best alpha that gives the maximum validation accuracy
best_alpha_index = np.argmax(val_scores)
best_alpha = ccp_alphas[best_alpha_index]

print(f"Best ccp_alpha for maximum validation accuracy: {best_alpha}")

# Train the decision tree with the best ccp_alpha
clf_best = DecisionTreeClassifier(random_state=42, ccp_alpha=best_alpha)
clf_best.fit(X_train, y_train)

# Plot training and validation scores vs. ccp_alpha
plt.figure(figsize=(10, 6))
plt.plot(ccp_alphas, train_scores, marker='o', label='Training Accuracy', drawstyle="steps-post")
plt.plot(ccp_alphas, val_scores, marker='o', label='Validation Accuracy', drawstyle="steps-post")
plt.xlabel("ccp_alpha")
plt.ylabel("Accuracy")
plt.title("Training and Validation Accuracy vs ccp_alpha")
plt.legend()
plt.show()

# Visualize the pruned decision tree
plt.figure(figsize=(20, 10))
plot_tree(clf_best, feature_names=X.columns, class_names=['0', '1'], filled=True, rounded=True)
plt.title(f"Decision Tree (Post-Pruned with ccp_alpha = {best_alpha})")
plt.show()

# Evaluate on the test set
y_pred = clf_best.predict(X_test)
test_accuracy = accuracy_score(y_test, y_pred)
print(f"Test accuracy with best ccp_alpha: {test_accuracy}")

# Confusion Matrix
cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,
            xticklabels=['Predicted 0', 'Predicted 1'],
            yticklabels=['Actual 0', 'Actual 1'])
plt.title('Confusion Matrix (Post-Pruned Decision Tree)')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

# ROC Curve
from sklearn.metrics import roc_curve, auc
fpr, tpr, thresholds = roc_curve(y_test, y_prob)
roc_auc = auc(fpr, tpr)

plt.figure(figsize=(10, 6))
plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC)')
plt.legend(loc="lower right")
plt.show()

"""### Bagging and Random Forest Classifiers after SMOTE"""

from sklearn.ensemble import BaggingClassifier, RandomForestClassifier
from sklearn.metrics import classification_report, roc_auc_score
from sklearn.model_selection import GridSearchCV
import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, auc

# Bagging Classifier with extended parameter grid
bagging_clf = BaggingClassifier(random_state=42)
bagging_params = {
    'n_estimators': [50, 100, 200],            # Increase n_estimators for stability
    'max_samples': [0.6, 0.8, 1.0],            # Adjust sampling size
    'max_features': [0.5, 0.75, 1.0],          # Add max_features to limit the number of features per base estimator
    'bootstrap': [True, False]                 # Consider bootstrapping or not
}
bagging_grid = GridSearchCV(bagging_clf, bagging_params, cv=5, scoring='roc_auc')
bagging_grid.fit(X_train, y_train)
bagging_best = bagging_grid.best_estimator_

# Random Forest Classifier with extended parameter grid
rf_clf = RandomForestClassifier(random_state=42)
rf_params = {
    'n_estimators': [100, 200, 300],           # Increase range of estimators
    'max_depth': [None, 10, 20, 30],           # Allow deeper trees if needed
    'min_samples_split': [2, 5, 10],           # Control minimum samples to split nodes
    'min_samples_leaf': [1, 2, 4],             # Control minimum samples for leaf nodes
    'max_features': ['sqrt', 'log2', None]     # Test different subsets of features
}
rf_grid = GridSearchCV(rf_clf, rf_params, cv=5, scoring='roc_auc')
rf_grid.fit(X_train, y_train)
rf_best = rf_grid.best_estimator_

# Predictions
bagging_pred = bagging_best.predict(X_test)
rf_pred = rf_best.predict(X_test)

# Performance Evaluation
print("Bagging Classifier:")
print("Best parameters:", bagging_grid.best_params_)
print(classification_report(y_test, bagging_pred))
print(f"ROC AUC Score: {roc_auc_score(y_test, bagging_best.predict_proba(X_test)[:, 1])}")

print("\nRandom Forest Classifier:")
print("Best parameters:", rf_grid.best_params_)
print(classification_report(y_test, rf_pred))
print(f"ROC AUC Score: {roc_auc_score(y_test, rf_best.predict_proba(X_test)[:, 1])}")

# Visualization (ROC Curve for both models)
plt.figure(figsize=(10, 6))

for clf, label in [(bagging_best, 'Bagging'), (rf_best, 'Random Forest')]:
    y_prob = clf.predict_proba(X_test)[:, 1]
    fpr, tpr, thresholds = roc_curve(y_test, y_prob)
    roc_auc = auc(fpr, tpr)
    plt.plot(fpr, tpr, label=f'{label} (AUC = {roc_auc:.2f})')

plt.plot([0, 1], [0, 1], 'k--')  # Random classifier line
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curves')
plt.legend(loc='lower right')
plt.show()

"""### Artificial Neural Network after SMOTE"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from tensorflow import keras
from tensorflow.keras import layers
from sklearn.metrics import confusion_matrix, roc_curve, auc
import matplotlib.pyplot as plt
import seaborn as sns

# Prepare the data
X = data4.drop('true_conversion', axis=1)
y = data4['true_conversion']

# Identify numerical and categorical features
numerical_cols = X.select_dtypes(include=np.number).columns
categorical_cols = X.select_dtypes(include=['object']).columns

# Create transformers
numerical_transformer = StandardScaler()
categorical_transformer = OneHotEncoder(handle_unknown="ignore")

# Combine transformers using ColumnTransformer
preprocessor = ColumnTransformer(
    transformers=[
        ('num', numerical_transformer, numerical_cols),
        ('cat', categorical_transformer, categorical_cols)
    ])

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create a pipeline
model_pipeline = Pipeline(steps=[('preprocessor', preprocessor)])

# Fit and transform the training data
X_train_transformed = model_pipeline.fit_transform(X_train)

# Transform the testing data
X_test_transformed = model_pipeline.transform(X_test)

# Define the ANN model
model = keras.Sequential([
    layers.Dense(128, input_shape=(X_train_transformed.shape[1],)),  # First dense layer
    layers.LeakyReLU(alpha=0.01),  # LeakyReLU with alpha=0.01
    layers.Dropout(0.2),  # Dropout layer for regularization
    layers.Dense(128),  # Second dense layer
    layers.LeakyReLU(alpha=0.01),  # LeakyReLU activation
    layers.Dropout(0.2),  # Dropout layer for regularization
    layers.Dense(1, activation='sigmoid')  # Output layer for binary classification
])

# Compile the model
model.compile(optimizer='adam',
              loss='binary_crossentropy',
              metrics=['accuracy'])

# Train the model
history = model.fit(X_train_transformed, y_train, epochs=100, batch_size=32, validation_split=0.1)

# Make predictions on the test set
y_pred_prob = model.predict(X_test_transformed)
y_pred = (y_pred_prob > 0.35).astype(int)

# Evaluate the model
loss, accuracy = model.evaluate(X_test_transformed, y_test, verbose=0)
print(f"Test Loss: {loss:.4f}")
print(f"Test Accuracy: {accuracy:.4f}")

# Confusion Matrix
cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,
            xticklabels=['Predicted 0', 'Predicted 1'],
            yticklabels=['Actual 0', 'Actual 1'])
plt.title('Confusion Matrix (ANN)')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

# ROC Curve
fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)
roc_auc = auc(fpr, tpr)

plt.figure(figsize=(10, 6))
plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ANN)')
plt.legend(loc="lower right")
plt.show()

# Plot training & validation accuracy values
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Model accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')
plt.show()

# Plot training & validation loss values
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')
plt.show()

"""### XG Boost after SMOTE"""

# Import necessary libraries
import pandas as pd
from sklearn.model_selection import train_test_split, RandomizedSearchCV
from xgboost import XGBClassifier
from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import LabelEncoder

# Separate features and target
X = data4.drop("true_conversion", axis=1)
y = data4["true_conversion"]

# Convert categorical features to numeric using one-hot encoding
X = pd.get_dummies(X, drop_first=True)

# Encode the target variable if it is categorical
le = LabelEncoder()
y = le.fit_transform(y)

# Split dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Define parameter grid for XGBoost
param_dist = {
    "n_estimators": [100, 200, 300, 400, 500],
    "max_depth": [3, 5, 7, 9, 11],
    "learning_rate": [0.01, 0.05, 0.1, 0.15, 0.2],
    "subsample": [0.6, 0.7, 0.8, 0.9, 1.0],
    "colsample_bytree": [0.6, 0.7, 0.8, 0.9, 1.0],
    "gamma": [0, 0.1, 0.2, 0.3, 0.4],
    "min_child_weight": [1, 3, 5, 7]
}

# Initialize XGBoost classifier
xgb_clf = XGBClassifier(use_label_encoder=False, eval_metric='logloss')

# Use RandomizedSearchCV to tune parameters
random_search = RandomizedSearchCV(
    estimator=xgb_clf,
    param_distributions=param_dist,
    n_iter=150,
    scoring='roc_auc',
    cv=10,
    verbose=1,
    n_jobs=-1,
    random_state=42
)

# Fit the model
random_search.fit(X_train, y_train)

# Get the best estimator
best_model = random_search.best_estimator_

# Make predictions
y_pred = best_model.predict(X_test)
y_pred_proba = best_model.predict_proba(X_test)[:, 1]

# Confusion Matrix
cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", cbar=False)
plt.title("Confusion Matrix")
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.show()

# ROC Curve
fpr, tpr, _ = roc_curve(y_test, y_pred_proba)
roc_auc = auc(fpr, tpr)

plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color="blue", label="ROC Curve (AUC = {:.2f})".format(roc_auc))
plt.plot([0, 1], [0, 1], color="red", linestyle="--")
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("Receiver Operating Characteristic (ROC) Curve")
plt.legend(loc="lower right")
plt.show()

# Print classification report
print(classification_report(y_test, y_pred))

"""# Dataset Class Balancing by ADASYN (Adaptive Synthetic Algorithm)"""

data3

from sklearn.utils import shuffle
from sklearn.preprocessing import LabelEncoder
from imblearn.over_sampling import SMOTE
import pandas as pd

# Assuming 'data3' is your DataFrame
# Separate features and target variable
X = data3.drop('true_conversion', axis=1)
y = data3['true_conversion']

# Identify numerical and categorical features
numerical_cols = X.select_dtypes(include=np.number).columns
categorical_cols = X.select_dtypes(exclude=np.number).columns

# Apply Label Encoding to categorical features
label_encoders = {}
for col in categorical_cols:
    le = LabelEncoder()
    X[col] = le.fit_transform(X[col])
    label_encoders[col] = le

# Apply SMOTE to balance the dataset
smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X, y)

# Create a new balanced DataFrame
data3_balanced = pd.DataFrame(X_resampled, columns=X.columns)
data3_balanced['true_conversion'] = y_resampled

# Now data3_balanced contains the balanced dataset with label-encoded categorical features
print("Balanced dataset class distribution:\n", data3_balanced['true_conversion'].value_counts())
data3_balanced

"""# Classification Models after ADASYN

### Decision Tree Model after ADASYN
"""

import pandas as pd
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, roc_curve, auc
import matplotlib.pyplot as plt
import seaborn as sns

# Prepare the data
X = data3_balanced.drop('true_conversion', axis=1)
y = data3_balanced['true_conversion']

# Convert categorical features to numerical using one-hot encoding
X = pd.get_dummies(X, drop_first=True)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize and train the decision tree classifier
clf = DecisionTreeClassifier(random_state=42)  # No max_depth for a fully grown tree
clf.fit(X_train, y_train)

# Visualize the decision tree
plt.figure(figsize=(40, 20))
plot_tree(clf, feature_names=X.columns, class_names=['0', '1'], filled=True, rounded=True)
plt.show()

# Make predictions on the test set
y_pred = clf.predict(X_test)
y_prob = clf.predict_proba(X_test)[:, 1]

# Evaluate the model
cm = confusion_matrix(y_test, y_pred)

# Plot confusion matrix heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,
            xticklabels=['Predicted 0', 'Predicted 1'],
            yticklabels=['Actual 0', 'Actual 1'])
plt.title('Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

# ROC curve
fpr, tpr, thresholds = roc_curve(y_test, y_prob)
roc_auc = auc(fpr, tpr)

plt.figure(figsize=(10, 6))
plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic')
plt.legend(loc="lower right")
plt.show()

"""### Pre-Pruned Decision Tree Model after ADASYN"""

from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.metrics import roc_auc_score, roc_curve, auc, confusion_matrix
from sklearn.model_selection import cross_val_score
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

# Initialize variables to store the best depth and the highest AUC score
best_depth = None
best_auc = 0
auc_scores = []

# Test max depths from 4 to 16
for depth in range(4, 16):
    clf = DecisionTreeClassifier(max_depth=depth, random_state=42)

    # Calculate the mean AUC score using cross-validation
    cv_auc = cross_val_score(clf, X_train, y_train, cv=5, scoring='roc_auc').mean()
    auc_scores.append(cv_auc)

    # Update best depth if current AUC is higher than the previous best
    if cv_auc > best_auc:
        best_auc = cv_auc
        best_depth = depth

# Prepare the data
X = data3_balanced.drop('true_conversion', axis=1)
y = data3_balanced['true_conversion']

# Convert categorical features to numerical using one-hot encoding
X = pd.get_dummies(X, drop_first=True)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train the decision tree with the best depth
clf_best = DecisionTreeClassifier(max_depth=best_depth, random_state=42)
clf_best.fit(X_train, y_train)

# Display the best depth
print(f"Best depth for maximum AUC: {best_depth}")

# Visualize the decision tree with the best depth
plt.figure(figsize=(20, 10))
plot_tree(clf_best, feature_names=X.columns, class_names=['0', '1'], filled=True, rounded=True)
plt.title(f"Decision Tree (Optimal Depth = {best_depth})")
plt.show()

# Make predictions and calculate AUC on the test set
y_pred = clf_best.predict(X_test)
y_prob = clf_best.predict_proba(X_test)[:, 1]
fpr, tpr, thresholds = roc_curve(y_test, y_prob)
roc_auc = auc(fpr, tpr)

# Confusion matrix
cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,
            xticklabels=['Predicted 0', 'Predicted 1'],
            yticklabels=['Actual 0', 'Actual 1'])
plt.title('Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

# Plot ROC curve
plt.figure(figsize=(10, 6))
plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic')
plt.legend(loc="lower right")
plt.show()

"""### Post-Pruned Decision Tree Model after ADASYN"""

import pandas as pd
import numpy as np
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import confusion_matrix, accuracy_score, roc_curve, auc
import matplotlib.pyplot as plt
import seaborn as sns

# Prepare the data
X = data3_balanced.drop('true_conversion', axis=1)
y = data3_balanced['true_conversion']

# Convert categorical features to numerical using one-hot encoding
X = pd.get_dummies(X, drop_first=True)

# Split the data into training and validation sets if not already done
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

# Fit a decision tree classifier to get the effective alphas
clf = DecisionTreeClassifier(random_state=42)
path = clf.cost_complexity_pruning_path(X_train, y_train)
ccp_alphas = path.ccp_alphas  # array of alpha values for pruning
clfs = []

# Train a series of decision trees with different ccp_alpha values
for ccp_alpha in ccp_alphas:
    clf = DecisionTreeClassifier(random_state=42, ccp_alpha=ccp_alpha)
    clf.fit(X_train, y_train)
    clfs.append(clf)

# Calculate training and validation accuracies for each tree
train_scores = [accuracy_score(y_train, clf.predict(X_train)) for clf in clfs]
val_scores = [accuracy_score(y_val, clf.predict(X_val)) for clf in clfs]

# Identify the best alpha that gives the maximum validation accuracy
best_alpha_index = np.argmax(val_scores)
best_alpha = ccp_alphas[best_alpha_index]

print(f"Best ccp_alpha for maximum validation accuracy: {best_alpha}")

# Train the decision tree with the best ccp_alpha
clf_best = DecisionTreeClassifier(random_state=42, ccp_alpha=best_alpha)
clf_best.fit(X_train, y_train)

# Plot training and validation scores vs. ccp_alpha
plt.figure(figsize=(10, 6))
plt.plot(ccp_alphas, train_scores, marker='o', label='Training Accuracy', drawstyle="steps-post")
plt.plot(ccp_alphas, val_scores, marker='o', label='Validation Accuracy', drawstyle="steps-post")
plt.xlabel("ccp_alpha")
plt.ylabel("Accuracy")
plt.title("Training and Validation Accuracy vs ccp_alpha")
plt.legend()
plt.show()

# Visualize the pruned decision tree
plt.figure(figsize=(20, 10))
plot_tree(clf_best, feature_names=X.columns, class_names=['0', '1'], filled=True, rounded=True)
plt.title(f"Decision Tree (Post-Pruned with ccp_alpha = {best_alpha})")
plt.show()

# Evaluate on the test set
y_pred = clf_best.predict(X_test)
test_accuracy = accuracy_score(y_test, y_pred)
print(f"Test accuracy with best ccp_alpha: {test_accuracy}")

# Confusion Matrix
cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,
            xticklabels=['Predicted 0', 'Predicted 1'],
            yticklabels=['Actual 0', 'Actual 1'])
plt.title('Confusion Matrix (Post-Pruned Decision Tree)')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

# ROC Curve
from sklearn.metrics import roc_curve, auc
fpr, tpr, thresholds = roc_curve(y_test, y_prob)
roc_auc = auc(fpr, tpr)

plt.figure(figsize=(10, 6))
plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC)')
plt.legend(loc="lower right")
plt.show()

"""### Naive Bayes Classifier after ADASYN"""

from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, roc_curve, auc
import matplotlib.pyplot as plt
import seaborn as sns

# Prepare the data
X = data3_balanced.drop('true_conversion', axis=1)
y = data3_balanced['true_conversion']

# Convert categorical features to numerical using one-hot encoding
X = pd.get_dummies(X, drop_first=True)

# Split the data into training and validation sets if not already done
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize and train the Gaussian Naive Bayes classifier
nb_classifier = GaussianNB()
nb_classifier.fit(X_train, y_train)

# Make predictions on the test set
y_pred = nb_classifier.predict(X_test)
y_prob = nb_classifier.predict_proba(X_test)[:, 1]

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
roc_auc = roc_auc_score(y_test, y_prob)


print(f"Accuracy: {accuracy}")
print(f"Precision: {precision}")
print(f"Recall: {recall}")
print(f"F1-score: {f1}")
print(f"ROC AUC: {roc_auc}")

# Confusion Matrix
cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,
            xticklabels=['Predicted 0', 'Predicted 1'],
            yticklabels=['Actual 0', 'Actual 1'])
plt.title('Confusion Matrix (Naive Bayes)')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

# ROC Curve
fpr, tpr, thresholds = roc_curve(y_test, y_prob)
roc_auc = auc(fpr, tpr)

plt.figure(figsize=(10, 6))
plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (Naive Bayes)')
plt.legend(loc="lower right")
plt.show()

"""### Bagging Classification Model after ADASYN"""

from sklearn.ensemble import BaggingClassifier
from sklearn.metrics import classification_report, roc_auc_score, roc_curve, auc, confusion_matrix
from sklearn.model_selection import GridSearchCV
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

# Separate features and target
X = data3_balanced.drop("true_conversion", axis=1)
y = data3_balanced["true_conversion"]

# Convert categorical features to numeric using one-hot encoding
X = pd.get_dummies(X, drop_first=True)

# Encode the target variable if it is categorical
le = LabelEncoder()
y = le.fit_transform(y)

# Split dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Bagging Classifier with extended parameter grid
bagging_clf = BaggingClassifier(random_state=42)
bagging_params = {
    'n_estimators': [50, 100, 200],
    'max_samples': [0.6, 0.8, 1.0],
    'max_features': [0.5, 0.75, 1.0],
    'bootstrap': [True, False]
}
bagging_grid = GridSearchCV(bagging_clf, bagging_params, cv=5, scoring='roc_auc')
bagging_grid.fit(X_train, y_train)
bagging_best = bagging_grid.best_estimator_

# Predictions and Evaluation
bagging_pred = bagging_best.predict(X_test)
bagging_prob = bagging_best.predict_proba(X_test)[:, 1]

# Print Best Parameters and Classification Report
print("Bagging Classifier:")
print("Best parameters:", bagging_grid.best_params_)
print(classification_report(y_test, bagging_pred))
print(f"ROC AUC Score: {roc_auc_score(y_test, bagging_prob)}")

# Confusion Matrix Heatmap
conf_matrix = confusion_matrix(y_test, bagging_pred)
plt.figure(figsize=(6, 4))
sns.heatmap(conf_matrix, annot=True, fmt="d", cmap="Blues", cbar=False)
plt.title("Confusion Matrix for Bagging Classifier")
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.show()

# ROC Curve
fpr, tpr, thresholds = roc_curve(y_test, bagging_prob)
roc_auc = auc(fpr, tpr)

plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, label=f'Bagging (AUC = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], 'k--')  # Random classifier line
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve for Bagging Classifier')
plt.legend(loc='lower right')
plt.show()

"""### Random Forest after ADASYN"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, roc_auc_score, roc_curve, auc, confusion_matrix
from sklearn.model_selection import GridSearchCV
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# Separate features and target
X = data3_balanced.drop("true_conversion", axis=1)
y = data3_balanced["true_conversion"]

# Convert categorical features to numeric using one-hot encoding
X = pd.get_dummies(X, drop_first=True)

# Encode the target variable if it is categorical
le = LabelEncoder()
y = le.fit_transform(y)

# Split dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Random Forest Classifier with extended parameter grid
rf_clf = RandomForestClassifier(random_state=42)
rf_params = {
    'n_estimators': [100, 200, 300],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'max_features': ['sqrt', 'log2', None]
}
rf_grid = GridSearchCV(rf_clf, rf_params, cv=5, scoring='roc_auc')
rf_grid.fit(X_train, y_train)
rf_best = rf_grid.best_estimator_

# Predictions and Evaluation
rf_pred = rf_best.predict(X_test)
rf_prob = rf_best.predict_proba(X_test)[:, 1]

# Print Best Parameters and Classification Report
print("\nRandom Forest Classifier:")
print("Best parameters:", rf_grid.best_params_)
print(classification_report(y_test, rf_pred))
print(f"ROC AUC Score: {roc_auc_score(y_test, rf_prob)}")

# Confusion Matrix Heatmap
conf_matrix_rf = confusion_matrix(y_test, rf_pred)
plt.figure(figsize=(6, 4))
sns.heatmap(conf_matrix_rf, annot=True, fmt="d", cmap="Greens", cbar=False)
plt.title("Confusion Matrix for Random Forest Classifier")
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.show()

# ROC Curve
fpr_rf, tpr_rf, thresholds_rf = roc_curve(y_test, rf_prob)
roc_auc_rf = auc(fpr_rf, tpr_rf)

plt.figure(figsize=(8, 6))
plt.plot(fpr_rf, tpr_rf, label=f'Random Forest (AUC = {roc_auc_rf:.2f})')
plt.plot([0, 1], [0, 1], 'k--')  # Random classifier line
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve for Random Forest Classifier')
plt.legend(loc='lower right')
plt.show()

# Feature Importance
rf_feature_importances = pd.Series(rf_best.feature_importances_, index=X_train.columns).sort_values(ascending=False)
print("\nFeature Importances (Random Forest):")
print(rf_feature_importances)

# Plot feature importances
plt.figure(figsize=(10, 6))
rf_feature_importances.plot(kind='bar')
plt.title("Feature Importances in Random Forest")
plt.xlabel("Features")
plt.ylabel("Importance Score")
plt.show()

"""### XG Boost after ADASYN"""

# Import necessary libraries
import pandas as pd
from sklearn.model_selection import train_test_split, RandomizedSearchCV
from xgboost import XGBClassifier
from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import LabelEncoder

# Separate features and target
X = data3_balanced.drop("true_conversion", axis=1)
y = data3_balanced["true_conversion"]

# Convert categorical features to numeric using one-hot encoding
X = pd.get_dummies(X, drop_first=True)

# Encode the target variable if it is categorical
le = LabelEncoder()
y = le.fit_transform(y)

# Split dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Define parameter grid for XGBoost
param_dist = {
    "n_estimators": [100, 200, 300, 400, 500],
    "max_depth": [3, 5, 7, 9, 11],
    "learning_rate": [0.01, 0.05, 0.1, 0.15, 0.2],
    "subsample": [0.6, 0.7, 0.8, 0.9, 1.0],
    "colsample_bytree": [0.6, 0.7, 0.8, 0.9, 1.0],
    "gamma": [0, 0.1, 0.2, 0.3, 0.4],
    "min_child_weight": [1, 3, 5, 7]
}

# Initialize XGBoost classifier
xgb_clf = XGBClassifier(use_label_encoder=False, eval_metric='logloss')

# Use RandomizedSearchCV to tune parameters
random_search = RandomizedSearchCV(
    estimator=xgb_clf,
    param_distributions=param_dist,
    n_iter=150,
    scoring='roc_auc',
    cv=10,
    verbose=1,
    n_jobs=-1,
    random_state=42
)

# Fit the model
random_search.fit(X_train, y_train)

# Get the best estimator
best_model = random_search.best_estimator_

# Make predictions
y_pred = best_model.predict(X_test)
y_pred_proba = best_model.predict_proba(X_test)[:, 1]

# Confusion Matrix
cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", cbar=False)
plt.title("Confusion Matrix")
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.show()

# ROC Curve
fpr, tpr, _ = roc_curve(y_test, y_pred_proba)
roc_auc = auc(fpr, tpr)

plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color="blue", label="ROC Curve (AUC = {:.2f})".format(roc_auc))
plt.plot([0, 1], [0, 1], color="red", linestyle="--")
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("Receiver Operating Characteristic (ROC) Curve")
plt.legend(loc="lower right")
plt.show()

# Print classification report
print(classification_report(y_test, y_pred))

"""### Artificial Neural Network after ADASYN"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from tensorflow import keras
from tensorflow.keras import layers
from sklearn.metrics import confusion_matrix, roc_curve, auc
import matplotlib.pyplot as plt
import seaborn as sns

# Prepare the data
X = data3_balanced.drop('true_conversion', axis=1)
y = data3_balanced['true_conversion']

# Identify numerical and categorical features
numerical_cols = X.select_dtypes(include=np.number).columns
categorical_cols = X.select_dtypes(include=['object']).columns

# Create transformers
numerical_transformer = StandardScaler()
categorical_transformer = OneHotEncoder(handle_unknown="ignore")

# Combine transformers using ColumnTransformer
preprocessor = ColumnTransformer(
    transformers=[
        ('num', numerical_transformer, numerical_cols),
        ('cat', categorical_transformer, categorical_cols)
    ])

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create a pipeline
model_pipeline = Pipeline(steps=[('preprocessor', preprocessor)])

# Fit and transform the training data
X_train_transformed = model_pipeline.fit_transform(X_train)

# Transform the testing data
X_test_transformed = model_pipeline.transform(X_test)

# Define the ANN model
model = keras.Sequential([
    layers.Dense(128, input_shape=(X_train_transformed.shape[1],)),  # First dense layer
    layers.LeakyReLU(alpha=0.01),  # LeakyReLU with alpha=0.01
    layers.Dropout(0.2),  # Dropout layer for regularization
    layers.Dense(128),  # Second dense layer
    layers.LeakyReLU(alpha=0.01),  # LeakyReLU activation
    layers.Dropout(0.2),  # Dropout layer for regularization
    layers.Dense(1, activation='sigmoid')  # Output layer for binary classification
])

# Compile the model
model.compile(optimizer='adam',
              loss='binary_crossentropy',
              metrics=['accuracy'])

# Train the model
history = model.fit(X_train_transformed, y_train, epochs=100, batch_size=32, validation_split=0.1)

# Make predictions on the test set
y_pred_prob = model.predict(X_test_transformed)
y_pred = (y_pred_prob > 0.35).astype(int)

# Evaluate the model
loss, accuracy = model.evaluate(X_test_transformed, y_test, verbose=0)
print(f"Test Loss: {loss:.4f}")
print(f"Test Accuracy: {accuracy:.4f}")

# Confusion Matrix
cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,
            xticklabels=['Predicted 0', 'Predicted 1'],
            yticklabels=['Actual 0', 'Actual 1'])
plt.title('Confusion Matrix (ANN)')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

# ROC Curve
fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)
roc_auc = auc(fpr, tpr)

plt.figure(figsize=(10, 6))
plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ANN)')
plt.legend(loc="lower right")
plt.show()

# Plot training & validation accuracy values
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Model accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')
plt.show()

# Plot training & validation loss values
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')
plt.show()

from google.colab import files

# Convert data2 DataFrame to CSV file
data2.to_csv('data2.csv', index=False)  # Use index=False if you don't need the index in the file.

# Download the CSV file
files.download('data2.csv')

#If data3_balanced exists as a dataframe
data3_balanced.to_csv('data3_balanced.csv', index=False)
files.download('data3_balanced.csv')